=====================================================================

BELOW IS A RUNNING STREAM OF NOTES WHICH DON'T HAVE TO MAKE ANY SENSE

=====================================================================

I/O is serialized with timing data

IIIIIIiiIIIIIIIiOOOOOoooOOOOooIIIIIiiIIII
|              ||     ||     ||         |

needs splitting the timing:
    both pieces start at the same time, but different messages, keeping order
    needs tracking absolute time of each piece to put in the next message
    can't format timing data right away, only when writing

    OR

    store last whitespace time, input/output position

    >25
    IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII
                 ^
    ^

We'd better not cut anything for the start



write window to log
write output to log
write input to log

read window from terminal
write window to slave

write output to terminal
write input to slave

read input from terminal
read output from slave

why do we even need to be terminated by a signal?
1. because we need to handle out-of-band SIGWINCH when waiting for output and
   waiting for input to be consumed
2. because we need to handle SIGINT, SIGTERM and SIGHUP timely to restore
   terminal, instead of simply dying

Can we suspend I/O transfer for writing the log entry?
We should - this provides flow control

However, if we delay data transfer, SIGWINCH might go out of sync with the
data. So some data in the kernel buffer and pending write on the slave will
be counting on one window size, while the actual window size will be different.

Do we really care about window size? Really?
Yes we do care about transferring it, so we might as well record it. *** Delay
with the transfer should coincide with delay in the log. *** But can it? We
have no idea if the slave has read all of its input buffers when it gets
SIGWINCH. So it will be out of sync. Still screen handles this anyway.

Can we do all this without terminating on a signal? With poll even on write?
Would it work on terminals?


WE NEED TO HANDLE INTERRUPTED I/O BECAUSE WE NEED TO HANDLE LATENCY LIMIT
TIMER

Log is always in UTF-8
Convert any other encoding to UTF-8
Write character counts in timing
Keep incomplete characters to the next write - don't flush them?
Or assume characters are always complete? THAT IS NOT TRUE!
OR WAIT! Perhaps it's our code breaking on incomplete characters?
VERIFY THIS!

Anyway, we can't be sure, we can have a buffer size which won't fit the
characters.

We have to assume I/O is encoded somehow and for UTF-8 it means multibyte
characters.

Valid UTF-8 is a smaller subset of arbitrary binary stream. Thus we can't
represent everything that's being pushed to the terminal.

However, if we need to have our terminal logs searcheable, we need that data
in a format that ElasticSearch can understand.

What if we add two more timing record types: binary input and binary output?
Have them encoded in base64?


Write both streams while delay is under 0, then flush their timing
What to do with interleaved binary data? We should keep the pieces before
flushing, or rather we should flush them as we switch forks.

Delay is put in by code above streams. Data lengths are put in by streams.

So when a new piece of input comes in, check if delay is still within zero.
If it is more than zero:
    Ask both streams to flush timing, giving them control of timing buffer,
    then append delay.
Hand off the newly received data to the corresponding stream giving it control
of remaining length and timing buffer

What should the stream reserve space for? Only "cutting" or "flushing" as well?
Only cutting, flushing will need more space.

if invalid byte encountered
    if sequence is empty
        write one byte from input buffer to binary fork
    else
        write binary sequence from utf8 buffer
else
    if sequence is incomplete
        add another byte
        if not added
            break
    else
        write text sequence from utf8 buffer

sequence incomplete and no invalid byte encountered: collecting sequence
or
invalid byte encountered: writing non-text sequence: either from utf8 buffer or one byte from the input buffer
or
sequence complete: writing text sequence from utf8 buffer

[ ] [ ] [ ] [ ] [ ]


ended   valid

0       0           sequence in progress

0       1           impossible

1       0           sequence terminated by invalid byte

1       1           sequence completed


ended   complete
0       0           sequence in progress
0       0           impossible
1       0           seqence terminated by invalid byte
1       1           sequence completed


sequence of bytes
    /      \
   /        \
 text     binary


while true
    if the sequence has ended
        If it is valid UTF-8
            finish writing it to text fork
        else
            finish writing it to binary fork
    get a sequence

the sequence:
binary or text
length
how much is written

What if we rename the current utf-8 object to a sequence object?
Then it doesn't have to contain only UTF-8 bytes. But would that be useful?

We only need to keep incomplete sequences between calls because UTF-8
characters are multi-byte and need to be written atomically.

What if we extend our sequence object to keeping words atomically?
We'll need to add word length limit as well. Otherwise the rest of the
machinery will be there already.

So, an incomplete text sequence can be kept between write calls, and can be
force-flushed.


------
Can we define the sequence?

A sequence can be started.
A sequence can be ended.

A sequence can be:

    binary,
    partial character (binary when incomplete),
    text (partial character when incomplete?),
    word (text when incomplete

A byte can belong to word, text, binary or unknown (partial character) sequence.


Sequence:

        __________________ character
      /              _____ word
     /              /
d0 b0 d0 b1 d0 b2 0a 32

a word is a character sequence
a character sequence is a binary sequence

------

        __________________ character
      /             ______ word
     /             /
d0 b0  d0 b1  d0 b2  0a


do we flush incomplete words?
do we flush incomplete characters?

store last position of a complete sequence type?



ff      binary started
        character not started
        word not started

d0      binary started
        character started
        word not started

Oa      binary started
        character complete
        word not started

d0 b0   binary started
        character complete
        word started

d0 b0  d0   binary started
            character started
            word started

d0 b0  d0 b1  d0 b2     binary started
                        character complete
                        word started

d0 b0  d0 b1  d0 b2     binary                  a0
                        character complete
                        word ended


A binary sequence is a sequence of any bytes

Non-character sequence is a binary sequence that doesn't contain a valid UTF-8
character.

Character sequence is a binary sequence that is a UTF-8 character.

Text sequence is a sequence of character sequences.

Word sequence is a text sequence that doesn't contain word-break characters.


--------------------------

How do we want to use it?


forever
    until we have a complete sequence
        add a byte to sequence
    write the sequence


        byte          character
atom    sequence      sequence

byte --- Non-character
     --- Character --- Non-word
                   --- Word

So, there are two dimensions: sequencing and alternating.

We can start with the first (byte) sequencing level and leave the second
(character) sequencing level for later.

A two level solution can be like this:

until we have a complete character sequence
    forever
        until the byte sequence ends
            add a byte to the sequence
        if it is a character
            add the character to the character sequence
            break
        write the bytes to binary fork
        empty the byte sequence
write the character sequence to the text fork
empty the character sequence


So a sequence object provides binary output: valid/non-valid, but only if the
sequence has ended.

top level stream writing functions return amount of data actually written
they write to their forks and also to global metadata
inside they need to do some atomic writes, sometimes to both forks at once

These second level functions can also accept the location for the remaining
length and write to the buffers opportunistically. However, when atomic
writing to a few buffers is necessary, we can have intermediate storage for
meta pointer and remaining length to ensure that we can roll-back the write
accounting

So on the second level at the moment we're writing a character, which can be
valid or invalid. A valid character is simply an atomic write to the text
fork. An invalid character is an atomic write both to text and to binary fork.
Either of them can write meta records when switching between valid and invalid
runs. These writes should also account for the necessary meta records, but not
write them. If the meta record cannot be fit the writing should say that no
data was written.

We need these operations:
    streams:
        tlog_stream_cut             write withheld incomplete characters
        tlog_stream_flush           write meta and reset runs
    io:
        tlog_io_cut                 write withheld incomplete stream characters
        tlog_io_flush               flush streams
    sink:
        tlog_sink_io_cut            write withheld incomplete I/O characters
        tlog_sink_io_flush          flush I/O and write it as a message

Abstract transaction:

    change some fields
    if some action changed
        rollback changes
        abort

Should we implement single transaction interface?
I.e. if a transaction started on upper levels, we don't start another one?
So there is no sub-transaction support? Would that be right? I.e. would there
ever be a transaction trying some sub-transaction variants, aborting them and
then settling on one variant, proceeding, and then aborting on a later action?
Nevertheless, can't we do this with a single transaction storage anyway?

An object should save only what makes sense to rollback according to all the
possible operations.

So, a tlog_io_write can look like this:
    tlog_io_trx_start(io)
        tlog_stream_trx_start(input)
        tlog_stream_trx_start(output)
    try writing delay to timing
    if failed:
        tlog_io_trx_abort(io)
            tlog_stream_trx_abort(input)
            tlog_stream_trx_abort(output)
    try writing stream
    if failed:
        tlog_io_trx_abort(io)
            tlog_stream_trx_abort(input)
            tlog_stream_trx_abort(output)
    tlog_io_trx_commit(io)
        tlog_stream_trx_commit(input)
        tlog_stream_trx_commit(output)


Copying the state should happen only at the top level transaction start.
Restoring the state should happen only at the top level transaction abort.
But this way we can't do sub-transactions. Yeah, and anyway, if we're doing
sub-transactions we're supposed to have another set of fields saved, so we can
roll-back properly. So, let's not try to support sub-transactions for now.
Let's just assume that only top level transactions save/rollback. So, we'll
need a transaction nesting counter. Any level can call start increasing it, or
abort/commit decreasing it. But only the top level will actually act on the
decision (abort/commit). Perhaps we can come up with a facility to make adding
transaction support to an object easier.

Ah, it seems we need sub-transactions for writing characters / binary pieces
atomically in streams. Or do we? Yeah, pretty much. We abort character /
binary piece writing, but we commit pieces written before these. I.e. we're
not aborting the whole stack.

Another challenge: pmeta. It's not a part of stream object, it's passed into
functions, all of them. But perhaps it should be? Then we can save it in a
transaction as well. The same about prem. Shan't we try implementing that
first? Hmm. It's not like they ever change, after all.

Ah, nope. While we can have these pointers kept in the stream objects for ease
of calling the object functions, we can't snapshot them as part of streams.
They're a part of I/O object and need to be preserved as part of its own
transactions, in tlog_io_write.

Alright, then the needed depth of the transaction stack is determined by the
upper layers. If they need a copy for their transactions, they're supposed to
allocate and own it. We shouldn't need to allocate anything dynamically.
Shall they be stored on the stack, then?

So let's store them on the stack, but copy there only if we need a
a new, or sub-transaction.

Let's define transaction storage type for each object type, which can simply
be the object type. Also let's define backup/restore operations for each
object, which can simply be a memcpy. Still, that storage type should be an
aggregate for the nested objects.

If a function can't fail, it shouldn't accept a transaction pointer.
If a function doesn't need to make backups and rollbacks, it shouldn't accept
a transaction pointer. However, every type expected to support transactions
should define backup/restore functions.

On a closer examination and attempting to do it, we'd better not pass pmeta
and prem to streams at creation time. This turns out to be more obscure and
harder to reason about.

If we remove separate window message, then we don't need separate packet type
- just supply window size each time, make the sink code or source user compare
  window sizes and detect changes. This can make the code much simpler.
OTOH, I/O data and window changes *do* come separately and such scheme would
require the user to cut the I/O themselves and take care of comparing window
sizes all the time. So, for a stream of events, having separate packet types,
as in "separate even types", makes sense. We can still have one message type
for the JSON sink.

We still need to preserve window size changes, likely. Can we put them inside
timing, as events? What would happen with the player, if there is no window
size at the start? How would the *partial* stream matches work?! Shall we
simply produce messages with empty I/O and new window sizes? These would
have timestamps. We shouldn't send such messages immediately, but only when
told to flush, or when we run out of I/O size limit. If window change comes
before I/O, the written message should have the window change time position,
and timing would have a delay record coming first - this needs a tlog_io
object change, so it starts when it's told it starts, not when first I/O comes
in. I.e. we can say that window change happens always at message start.

No, this is wrong. This means that we won't be able to merge messages across
window changes. To satisfy both the need to merge messages and the need to
process partial streams, window changes should be able to appear any place in
the message (i.e. in "timing"), and each message should have a window size
record at the start. Argh.

BTW, how are we even going to merge particular stream chunks with audit
messages for a specific session? Could Kibana plugin request audit data by
itself? Not likely, and it wouldn't be right, anyway. We can tell users:
search your messages and then add 'OR something that matches a bunch of
relevant audit messages' to the query, and the playback visualisation would
filter them out by sessionid and time ranges. That's ugly. Do we have
alternatives? I think it is time to actually try making a visualisation.

Currently partial characters written with a delay produce extra delay records
in timing. This happens because chunk assumes that every write results in a
timing record, whereas partial character writes don't produce them.

The solution might be to not write any delay record until an actual attempt to
write a timing record occurs. That means that instead of writing a delay
record before each first arriving packet, we need to reserve space for the
future record and write it later.

We need to review how the timing should actually happen:

We need to write timing *before* any actual stream write or new window size
that happened at least one millisecond from the last.

What if we had a separate object/module called "timing" that dealt with
writing delays and other records appropriately? And also dealt with
reserving the space for the future delay records?

We have to decide how to manage rollbacks in the general case.

A rollback is necessary to maintain consistency.

An object only stores itself and contained objects in a transaction.  It's not
a business of the object to store any objects that don't belong to it. OTOH,
it is leaving outside objects inconsistent regarding its own state.  So, if an
object wants to keep its own and other state consistent, it should preserve
the outside objects as well.

The problem with this is that outside objects might have already been saved
for this transaction upstack and we'd better avoid doing extra copies.

There can be many (sub-)transactions happening on the same object.

Or can we simply assume that a transaction will have all the necessary objects
included upstack and just care about ones we directly own?  OTOH, then why do
we even care passing a transaction state around if all saving/restoring should
be done in a single stack frame?

Or shall we perhaps not worry about copies and just do them anyway?  However,
that should be done in a sub-transaction.  Shall we get rid of transaction IDs
then?  Otherwise we would need to maintain a knowledge of which objects were
backed up and which weren't.

Perhaps we can limit the number of active transactions and have a bitmap in
each object saying which transaction has it preserved?

For now we can just assume all backup/restore is happening in a single place
and preserve transaction state as a future feature.
